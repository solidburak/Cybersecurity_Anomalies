{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51be1f3d-e425-4d6d-9c05-fb6d98664c68",
   "metadata": {},
   "source": [
    "![cyber_photo](cyber_photo.jpg)\n",
    "\n",
    "Cyber threats are a growing concern for organizations worldwide. These threats take many forms, including malware, phishing, and denial-of-service (DOS) attacks, compromising sensitive information and disrupting operations. The increasing sophistication and frequency of these attacks make it imperative for organizations to adopt advanced security measures. Traditional threat detection methods often fall short due to their inability to adapt to new and evolving threats. This is where deep learning models come into play.\n",
    "\n",
    "Deep learning models can analyze vast amounts of data and identify patterns that may not be immediately obvious to human analysts. By leveraging these models, organizations can proactively detect and mitigate cyber threats, safeguarding their sensitive information and ensuring operational continuity.\n",
    "\n",
    "As a cybersecurity analyst, you identify and mitigate these threats. In this project, you will design and implement a deep learning model to detect cyber threats. The BETH dataset simulates real-world logs, providing a rich source of information for training and testing your model. The data has already undergone preprocessing, and we have a target label, `sus_label`, indicating whether an event is malicious (1) or benign (0).\n",
    "\n",
    "By successfully developing this model, you will contribute to enhancing cybersecurity measures and protecting organizations from potentially devastating cyber attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8811256f-f887-4867-903e-837238fbb648",
   "metadata": {},
   "source": [
    "\n",
    "### The Data\n",
    "\n",
    "| Column     | Description              |\n",
    "|------------|--------------------------|\n",
    "|`processId`|The unique identifier for the process that generated the event - int64 |\n",
    "|`threadId`|ID for the thread spawning the log - int64|\n",
    "|`parentProcessId`|Label for the process spawning this log - int64|\n",
    "|`userId`|ID of user spawning the log|Numerical - int64|\n",
    "|`mountNamespace`|Mounting restrictions the process log works within - int64|\n",
    "|`argsNum`|Number of arguments passed to the event - int64|\n",
    "|`returnValue`|Value returned from the event log (usually 0) - int64|\n",
    "|`sus_label`|Binary label as suspicous event (1 is suspicious, 0 is not) - int64|\n",
    "\n",
    "More information on the dataset: [BETH dataset](accreditation.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75892dec-9424-4c92-bf8e-2f9847b7d7cd",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 54,
    "lastExecutedAt": 1769379532461,
    "lastExecutedByKernel": "78867116-bed0-4f87-b19b-2b0f2b4479d5",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#¬†Import required libraries\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as functional\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.optim as optim\nfrom torchmetrics import Accuracy\n# from sklearn.metrics import accuracy_score  # uncomment to use sklearn"
   },
   "outputs": [],
   "source": [
    "#¬†Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy\n",
    "# from sklearn.metrics import accuracy_score  # uncomment to use sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a009eb-5635-4bc9-90a9-ccf515481e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import multiprocessing\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Use 'spawn' to avoid CUDA initialization errors in workers\n",
    "    multiprocessing.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "938f75ab-4340-47d9-83e6-6aba9c86d81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Path: /home/burak/Documents/Codes/Python/Cypersecurity_Anomalies/.venv/bin/python3\n",
      "Is CUDA available: True\n",
      "Torch version: 2.10.0+cu130\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "print(f\"Python Path: {sys.executable}\")\n",
    "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e52e231f-71b5-4dfc-81f9-a3321ff78047",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 268,
    "lastExecutedAt": 1769379532729,
    "lastExecutedByKernel": "78867116-bed0-4f87-b19b-2b0f2b4479d5",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#¬†Load preprocessed data\ntrain_df = pd.read_csv('labelled_train.csv')\ntest_df = pd.read_csv('labelled_test.csv')\nval_df = pd.read_csv('labelled_validation.csv')\n\n# View the first 5 rows of training set+\ntrain_df.head()",
    "outputsMetadata": {
     "0": {
      "height": 550,
      "tableState": {},
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processId</th>\n",
       "      <th>threadId</th>\n",
       "      <th>parentProcessId</th>\n",
       "      <th>userId</th>\n",
       "      <th>mountNamespace</th>\n",
       "      <th>argsNum</th>\n",
       "      <th>returnValue</th>\n",
       "      <th>sus_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>381</td>\n",
       "      <td>7337</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>4026532231</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>381</td>\n",
       "      <td>7337</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>4026532231</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>381</td>\n",
       "      <td>7337</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>4026532231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7347</td>\n",
       "      <td>7347</td>\n",
       "      <td>7341</td>\n",
       "      <td>0</td>\n",
       "      <td>4026531840</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7347</td>\n",
       "      <td>7347</td>\n",
       "      <td>7341</td>\n",
       "      <td>0</td>\n",
       "      <td>4026531840</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   processId  threadId  parentProcessId  userId  mountNamespace  argsNum  \\\n",
       "0        381      7337                1     100      4026532231        5   \n",
       "1        381      7337                1     100      4026532231        1   \n",
       "2        381      7337                1     100      4026532231        0   \n",
       "3       7347      7347             7341       0      4026531840        2   \n",
       "4       7347      7347             7341       0      4026531840        4   \n",
       "\n",
       "   returnValue  sus_label  \n",
       "0            0          1  \n",
       "1            0          1  \n",
       "2            0          1  \n",
       "3           -2          1  \n",
       "4            0          1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#¬†Load preprocessed data\n",
    "train_df = pd.read_csv('./data/labelled_train.csv')\n",
    "test_df = pd.read_csv('./data/labelled_test.csv')\n",
    "val_df = pd.read_csv('./data/labelled_validation.csv')\n",
    "\n",
    "# View the first 5 rows of training set+\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b7ee389-d5ef-4c4d-bc7a-4d0be0e1c6e1",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 3664,
    "lastExecutedAt": 1769373855447,
    "lastExecutedByKernel": "78867116-bed0-4f87-b19b-2b0f2b4479d5",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Start coding here\n# Use as many cells as you need\nimport torch\nimport torch.nn as nn\n\nclass MalwareDetector(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # --- 1. Embedding Layers ---\n        # (num_embeddings, embedding_dim)\n        # We add +1 to sizes for the \"Unknown\" bucket at index 0\n        self.emb_process = nn.Embedding(546 + 1, 4) \n        # Note: We'll need to confirm the exact max IDs for thread/parent later, \n        # but for now we'll stick to the placeholder sizes we discussed.\n        self.emb_thread = nn.Embedding(1000 + 1, 4) \n        self.emb_parent = nn.Embedding(1000 + 1, 4) \n        self.emb_return = nn.Embedding(524 + 1, 4)\n        \n        # --- 2. The Main Network ---\n        self.input_dim = 30 # Calculated: 16 (embs) + 7 (user) + 6 (mount) + 1 (args)\n        \n        self.layer1 = nn.Linear(self.input_dim, 16)\n        self.layer2 = nn.Linear(16, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, process_id, thread_id, parent_id, return_val, user_oh, mount_oh, args_num):\n        # 1. Get Embeddings\n        e1 = self.emb_process(process_id)\n        e2 = self.emb_thread(thread_id)\n        e3 = self.emb_parent(parent_id)\n        e4 = self.emb_return(return_val)\n        \n        # 2. Reshape scalar to (Batch, 1)\n        args_num = args_num.view(-1, 1)\n        \n        # 3. Concatenate Everything\n        x = torch.cat([e1, e2, e3, e4, user_oh, mount_oh, args_num], dim=1)\n        \n        # 4. Pass through layers\n        x = self.layer1(x)\n        x = torch.relu(x)\n        x = self.layer2(x)\n        x = self.sigmoid(x)\n        \n        return x",
    "outputsMetadata": {
     "0": {
      "height": 50,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "016bea17-e711-4e63-9b6a-31901dcaac88",
        "nodeType": "const"
       }
      },
      "type": "dataFrame"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Start coding here\n",
    "# Use as many cells as you need\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MalwareDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- 1. Embedding Layers ---\n",
    "        # (num_embeddings, embedding_dim)\n",
    "        # We add +1 to sizes for the \"Unknown\" bucket at index 0\n",
    "        self.emb_process = nn.Embedding(546 + 1, 4) \n",
    "        # Note: We'll need to confirm the exact max IDs for thread/parent later, \n",
    "        # but for now we'll stick to the placeholder sizes we discussed.\n",
    "        self.emb_thread = nn.Embedding(1000 + 1, 4) \n",
    "        self.emb_parent = nn.Embedding(1000 + 1, 4) \n",
    "        self.emb_return = nn.Embedding(524 + 1, 4)\n",
    "\n",
    "        # -- Define the Dropout Layer üõë\n",
    "        # We drop 50% of the embeddings to force the model to look elsewhere.\n",
    "        self.emb_dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # --- 2. The Main Network ---\n",
    "        self.input_dim = 30 # Calculated: 16 (embs) + 7 (user) + 6 (mount) + 1 (args)\n",
    "        \n",
    "        self.layer1 = nn.Linear(self.input_dim, 16)\n",
    "        self.layer2 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, process_id, thread_id, parent_id, return_val, user_oh, mount_oh, args_num):\n",
    "        # 1. Get Embeddings\n",
    "        e1 = self.emb_process(process_id)\n",
    "        e2 = self.emb_thread(thread_id)\n",
    "        e3 = self.emb_parent(parent_id)\n",
    "        e4 = self.emb_return(return_val)\n",
    "        \n",
    "        # 2. Reshape scalar to (Batch, 1)\n",
    "        args_num = args_num.view(-1, 1)\n",
    "        \n",
    "        # 3. Concatenate Everything\n",
    "        x = torch.cat([e1, e2, e3, e4, user_oh, mount_oh, args_num], dim=1)\n",
    "\n",
    "        # --- Apply Dropout Here ---\n",
    "        # This randomly zeros out parts of the input vector\n",
    "        x = self.emb_dropout(x)\n",
    "        \n",
    "        # 4. Pass through layers\n",
    "        x = self.layer1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2292bcd0-dcef-4553-9e47-4b49483447e8",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1769379532832,
    "lastExecutedByKernel": "78867116-bed0-4f87-b19b-2b0f2b4479d5",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "list(sorted(train_df[\"mountNamespace\"].unique()))"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.int64(4026531840),\n",
       " np.int64(4026532217),\n",
       " np.int64(4026532229),\n",
       " np.int64(4026532231),\n",
       " np.int64(4026532232),\n",
       " np.int64(4026532288)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted(train_df[\"mountNamespace\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a922115-8f1e-4e40-96e2-799c333d670e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 3426,
    "lastExecutedAt": 1769379536258,
    "lastExecutedByKernel": "78867116-bed0-4f87-b19b-2b0f2b4479d5",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import torch\nimport numpy as np\nimport pandas as pd\n\n# --- Configuration ---\n# We define our exact known categories here to ensure fixed input sizes\nKNOWN_USERS = list(sorted(train_df[\"userId\"].unique()))  # Example list of 7 user IDs\nKNOWN_MOUNTS = list(sorted(train_df[\"mountNamespace\"].unique())) # Example 6 mounts\n\ndef build_vocab(df, columns):\n    \"\"\"\n    Builds indices for Embedding columns.\n    Returns: { 'col_name': { 'value': index, ... } }\n    \"\"\"\n    vocab = {}\n    for col in columns:\n        unique_vals = df[col].unique()\n        # Start index at 1, reserving 0 for 'Unknown'\n        vocab[col] = {val: i + 1 for i, val in enumerate(unique_vals)}\n    return vocab\n\ndef one_hot_encode(val, known_categories):\n    \"\"\"\n    Manually creates a One-Hot vector to ensure fixed size.\n    If val is not in known_categories, returns a vector of all zeros (or a specific unknown index).\n    \"\"\"\n    # Create a vector of zeros with length equal to our known categories\n    vec = np.zeros(len(known_categories), dtype=np.float32)\n    \n    try:\n        # Find the position of the current value\n        idx = known_categories.index(val)\n        vec[idx] = 1.0\n    except ValueError:\n        # If value is not in our known list, we leave it as all zeros \n        # (This acts as an implicit \"Other\" bucket)\n        pass \n        \n    return vec\n\ndef preprocess_data(df, vocab_maps):\n    \"\"\"\n    Converts a raw DataFrame into the exact Tensors the model needs.\n    \"\"\"\n    \n    # --- 1. Process Embeddings (Indices) ---\n    # We use .apply to map every value to its index, defaulting to 0\n    proc_idxs = df['processId'].apply(lambda x: vocab_maps['processId'].get(x, 0)).values\n    thread_idxs = df['threadId'].apply(lambda x: vocab_maps['threadId'].get(x, 0)).values\n    parent_idxs = df['parentProcessId'].apply(lambda x: vocab_maps['parentProcessId'].get(x, 0)).values\n    return_idxs = df['returnValue'].apply(lambda x: vocab_maps['returnValue'].get(x, 0)).values\n    \n    # Convert to LongTensors (integers) for Embedding layers\n    t_process = torch.tensor(proc_idxs, dtype=torch.long)\n    t_thread = torch.tensor(thread_idxs, dtype=torch.long)\n    t_parent = torch.tensor(parent_idxs, dtype=torch.long)\n    t_return = torch.tensor(return_idxs, dtype=torch.long)\n    \n    # --- 2. Process One-Hots (Floats) ---\n    # We apply our strict one_hot_encode function\n    # Result is a list of arrays, so we stack them into a 2D matrix\n    user_list = df['userId'].apply(lambda x: one_hot_encode(x, KNOWN_USERS)).tolist()\n    mount_list = df['mountNamespace'].apply(lambda x: one_hot_encode(x, KNOWN_MOUNTS)).tolist()\n    \n    t_user = torch.tensor(user_list, dtype=torch.float32)   # Shape: (Batch, 7)\n    t_mount = torch.tensor(mount_list, dtype=torch.float32) # Shape: (Batch, 6)\n    \n    # --- 3. Process Scalar (Log1p + Scale) ---\n    # Apply log1p: log(1 + x)\n    args_log = np.log1p(df['argsNum'].values)\n    \n    # Optional: If you want to scale it further (e.g. divide by max), do it here.\n    # For now, raw log1p is usually safe enough.\n    t_args = torch.tensor(args_log, dtype=torch.float32) # Shape: (Batch)\n    \n    # --- 4. Get Labels ---\n    t_labels = torch.tensor(df['sus_label'].values, dtype=torch.float32).view(-1, 1)\n    \n    return t_process, t_thread, t_parent, t_return, t_user, t_mount, t_args, t_labels\n\n# Usage Example:\n# 1. Setup Vocab (Do this ONCE with your training data)\nemb_cols = ['processId', 'threadId', 'parentProcessId', 'returnValue']\nvocab_maps = build_vocab(train_df, emb_cols)\n\n# 2. Convert Data\ninputs = preprocess_data(train_df, vocab_maps)"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16189/1046800769.py:65: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
      "  t_user = torch.tensor(user_list, dtype=torch.float32)   # Shape: (Batch, 7)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "# We define our exact known categories here to ensure fixed input sizes\n",
    "KNOWN_USERS = list(sorted(train_df[\"userId\"].unique()))  # Example list of 7 user IDs\n",
    "KNOWN_MOUNTS = list(sorted(train_df[\"mountNamespace\"].unique())) # Example 6 mounts\n",
    "\n",
    "def build_vocab(df, columns):\n",
    "    \"\"\"\n",
    "    Builds indices for Embedding columns.\n",
    "    Returns: { 'col_name': { 'value': index, ... } }\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for col in columns:\n",
    "        unique_vals = df[col].unique()\n",
    "        # Start index at 1, reserving 0 for 'Unknown'\n",
    "        vocab[col] = {val: i + 1 for i, val in enumerate(unique_vals)}\n",
    "    return vocab\n",
    "\n",
    "def one_hot_encode(val, known_categories):\n",
    "    \"\"\"\n",
    "    Manually creates a One-Hot vector to ensure fixed size.\n",
    "    If val is not in known_categories, returns a vector of all zeros (or a specific unknown index).\n",
    "    \"\"\"\n",
    "    # Create a vector of zeros with length equal to our known categories\n",
    "    vec = np.zeros(len(known_categories), dtype=np.float32)\n",
    "    \n",
    "    try:\n",
    "        # Find the position of the current value\n",
    "        idx = known_categories.index(val)\n",
    "        vec[idx] = 1.0\n",
    "    except ValueError:\n",
    "        # If value is not in our known list, we leave it as all zeros \n",
    "        # (This acts as an implicit \"Other\" bucket)\n",
    "        pass \n",
    "        \n",
    "    return vec\n",
    "\n",
    "def preprocess_data(df, vocab_maps):\n",
    "    \"\"\"\n",
    "    Converts a raw DataFrame into the exact Tensors the model needs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Process Embeddings (Indices) ---\n",
    "    # We use .apply to map every value to its index, defaulting to 0\n",
    "    proc_idxs = df['processId'].apply(lambda x: vocab_maps['processId'].get(x, 0)).values\n",
    "    thread_idxs = df['threadId'].apply(lambda x: vocab_maps['threadId'].get(x, 0)).values\n",
    "    parent_idxs = df['parentProcessId'].apply(lambda x: vocab_maps['parentProcessId'].get(x, 0)).values\n",
    "    return_idxs = df['returnValue'].apply(lambda x: vocab_maps['returnValue'].get(x, 0)).values\n",
    "    \n",
    "    # Convert to LongTensors (integers) for Embedding layers\n",
    "    t_process = torch.tensor(proc_idxs, dtype=torch.long)\n",
    "    t_thread = torch.tensor(thread_idxs, dtype=torch.long)\n",
    "    t_parent = torch.tensor(parent_idxs, dtype=torch.long)\n",
    "    t_return = torch.tensor(return_idxs, dtype=torch.long)\n",
    "    \n",
    "    # --- 2. Process One-Hots (Floats) ---\n",
    "    # We apply our strict one_hot_encode function\n",
    "    # Result is a list of arrays, so we stack them into a 2D matrix\n",
    "    user_list = df['userId'].apply(lambda x: one_hot_encode(x, KNOWN_USERS)).tolist()\n",
    "    mount_list = df['mountNamespace'].apply(lambda x: one_hot_encode(x, KNOWN_MOUNTS)).tolist()\n",
    "    \n",
    "    t_user = torch.tensor(user_list, dtype=torch.float32)   # Shape: (Batch, 7)\n",
    "    t_mount = torch.tensor(mount_list, dtype=torch.float32) # Shape: (Batch, 6)\n",
    "    \n",
    "    # --- 3. Process Scalar (Log1p + Scale) ---\n",
    "    # Apply log1p: log(1 + x)\n",
    "    args_log = np.log1p(df['argsNum'].values)\n",
    "    \n",
    "    # Optional: If you want to scale it further (e.g. divide by max), do it here.\n",
    "    # For now, raw log1p is usually safe enough.\n",
    "    t_args = torch.tensor(args_log, dtype=torch.float32) # Shape: (Batch)\n",
    "    \n",
    "    # --- 4. Get Labels ---\n",
    "    t_labels = torch.tensor(df['sus_label'].values, dtype=torch.float32).view(-1, 1)\n",
    "    \n",
    "    return t_process, t_thread, t_parent, t_return, t_user, t_mount, t_args, t_labels\n",
    "\n",
    "# Usage Example:\n",
    "# 1. Setup Vocab (Do this ONCE with your training data)\n",
    "emb_cols = ['processId', 'threadId', 'parentProcessId', 'returnValue']\n",
    "vocab_maps = build_vocab(train_df, emb_cols)\n",
    "\n",
    "# 2. Convert Data\n",
    "inputs = preprocess_data(train_df, vocab_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f67fbd05-ed22-497d-bdd1-8de7dd4e8370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move your model\n",
    "model = MalwareDetector().to(device)\n",
    "\n",
    "# Move your data inside the training loop\n",
    "inputs = tuple([inpt.to(device, non_blocking=True) for inpt in inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8d57ffa-ff44-4b3d-bed7-3f6a6640b52b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 311,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7f9cb8ebe0c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/burak/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/multiprocessing/connection.py\", line 133, in __del__\n",
      "    self._close()\n",
      "  File \"/home/burak/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/multiprocessing/connection.py\", line 377, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | Loss: 70.9879\n",
      "Epoch 2/12 | Loss: 6.0863\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Iterate over batches\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Unpack the batch (matches the order returned by preprocess_data)\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mb_proc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_parent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_ret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_user\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_mount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_labels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 1. Ensure labels are flat (1D) before we start\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codes/Python/Cypersecurity_Anomalies/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:741\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    740\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    744\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    745\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    746\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    747\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codes/Python/Cypersecurity_Anomalies/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:801\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    800\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m801\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    803\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codes/Python/Cypersecurity_Anomalies/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     52\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     56\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codes/Python/Cypersecurity_Anomalies/.venv/lib/python3.12/site-packages/torch/utils/data/dataset.py:206\u001b[39m, in \u001b[36mTensorDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codes/Python/Cypersecurity_Anomalies/.venv/lib/python3.12/site-packages/torch/utils/data/dataset.py:206\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tensors)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(*inputs) \n",
    "# batch_size=2 means we process 2 rows at a time\n",
    "dataloader = DataLoader(dataset, batch_size=2048, shuffle=True, num_workers=0, pin_memory=False)\n",
    "\n",
    "# --- 4. Initialize Model & Training Tools ---\n",
    "model = MalwareDetector().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# --- 5. The Training Loop üèãÔ∏è ---\n",
    "num_epochs = 12\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for batch in dataloader:\n",
    "        # Unpack the batch (matches the order returned by preprocess_data)\n",
    "        b_proc, b_thread, b_parent, b_ret, b_user, b_mount, b_args, b_labels = batch\n",
    "\n",
    "        # 1. Ensure labels are flat (1D) before we start\n",
    "        labels_flat = b_labels.squeeze()\n",
    "        # Generate random probabilities for the \"coin flip\"\n",
    "        probs = torch.rand_like(labels_flat, dtype=torch.float)\n",
    "        # Create the mask: (Is Malicious) AND (Coin Flip < 50%)\n",
    "        mask = (probs < 0.13)\n",
    "        # Overwrite the Parent Process ID with 0 (Unknown) where mask is True\n",
    "        b_parent[mask] = 0\n",
    "        #b_ret[mask] = 0\n",
    "\n",
    "        \n",
    "        # A. Zero Gradients (Clean the workspace!)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # B. Forward Pass\n",
    "        outputs = model(b_proc, b_thread, b_parent, b_ret, b_user, b_mount, b_args)\n",
    "        \n",
    "        # C. Calculate Loss\n",
    "        loss = criterion(outputs, b_labels)\n",
    "        \n",
    "        # D. Backward Pass (Calculate gradients)\n",
    "        loss.backward()\n",
    "        \n",
    "        # E. Update Weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146ae267-9cf6-4c59-b49f-effda163b2ce",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 13,
    "lastExecutedAt": 1769382120548,
    "lastExecutedByKernel": "78867116-bed0-4f87-b19b-2b0f2b4479d5",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n\ndef evaluate_model(model, dataloader, threshold=0.5):\n    \"\"\"\n    Runs the model on the provided dataloader and calculates performance metrics.\n    \n    Args:\n        model: The trained PyTorch model.\n        dataloader: The validation or test DataLoader.\n        threshold: The cutoff for classifying as Malicious (default 0.5).\n        \n    Returns:\n        metrics: A dictionary containing Accuracy, Precision, Recall, F1, and the Confusion Matrix.\n    \"\"\"\n    model.eval() # Set model to evaluation mode (turns off Dropout, etc.)\n    \n    all_labels = []\n    all_preds = []\n    \n    # 1. Disable Gradient Calculation (Save Memory/Speed)\n    with torch.no_grad():\n        for batch in dataloader:\n            # Unpack batch (Adjust indices based on your exact preprocess_data return)\n            b_proc, b_thread, b_parent, b_ret, b_user, b_mount, b_args, b_labels = batch\n            \n            # 2. Get Raw Predictions (Probabilities)\n            outputs = model(b_proc, b_thread, b_parent, b_ret, b_user, b_mount, b_args)\n            \n            # 3. Collect Labels and Predictions\n            all_labels.extend(b_labels.cpu().numpy())\n            all_preds.extend(outputs.cpu().numpy())\n\n    # 4. Convert lists to numpy arrays\n    # Flatten arrays to shape (N,) instead of (N, 1)\n    y_true = np.array(all_labels).flatten()\n    y_probs = np.array(all_preds).flatten()\n    \n    # 5. Apply Threshold to get Binary 0 or 1\n    y_pred_bin = (y_probs >= threshold).astype(int)\n    \n    # 6. Calculate Metrics\n    metrics = {\n        'accuracy': accuracy_score(y_true, y_pred_bin),\n        'precision': precision_score(y_true, y_pred_bin, zero_division=0),\n        'recall': recall_score(y_true, y_pred_bin, zero_division=0),\n        'f1': f1_score(y_true, y_pred_bin, zero_division=0),\n        'conf_matrix': confusion_matrix(y_true, y_pred_bin)\n    }\n    \n    return metrics\n\n# --- Usage Example ---\n# Assuming you have a 'val_loader' ready\n# results = evaluate_model(model, val_loader)\n# print(f\"F1 Score: {results['f1']:.4f}\")"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, dataloader, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Runs the model on the provided dataloader and calculates performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained PyTorch model.\n",
    "        dataloader: The validation or test DataLoader.\n",
    "        threshold: The cutoff for classifying as Malicious (default 0.5).\n",
    "        \n",
    "    Returns:\n",
    "        metrics: A dictionary containing Accuracy, Precision, Recall, F1, and the Confusion Matrix.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode (turns off Dropout, etc.)\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    # 1. Disable Gradient Calculation (Save Memory/Speed)\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Unpack batch (Adjust indices based on your exact preprocess_data return)\n",
    "            b_proc, b_thread, b_parent, b_ret, b_user, b_mount, b_args, b_labels = batch\n",
    "            \n",
    "            # 2. Get Raw Predictions (Probabilities)\n",
    "            outputs = model(b_proc, b_thread, b_parent, b_ret, b_user, b_mount, b_args)\n",
    "            \n",
    "            # 3. Collect Labels and Predictions\n",
    "            all_labels.extend(b_labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    # 4. Convert lists to numpy arrays\n",
    "    # Flatten arrays to shape (N,) instead of (N, 1)\n",
    "    y_true = np.array(all_labels).flatten()\n",
    "    y_probs = np.array(all_preds).flatten()\n",
    "    \n",
    "    # 5. Apply Threshold to get Binary 0 or 1\n",
    "    y_pred_bin = (y_probs >= threshold).astype(int)\n",
    "    \n",
    "    # 6. Calculate Metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred_bin),\n",
    "        'precision': precision_score(y_true, y_pred_bin, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred_bin, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred_bin, zero_division=0),\n",
    "        'conf_matrix': confusion_matrix(y_true, y_pred_bin)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# --- Usage Example ---\n",
    "# Assuming you have a 'val_loader' ready\n",
    "# results = evaluate_model(model, val_loader)\n",
    "# print(f\"F1 Score: {results['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66819d64-8de5-49fc-a568-b709cdeb5359",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 10891,
    "lastExecutedAt": 1769385858461,
    "lastExecutedByKernel": "78867116-bed0-4f87-b19b-2b0f2b4479d5",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#vocab_maps = build_vocab(train_df, emb_cols)\n\ninputs = preprocess_data(val_df, vocab_maps)\n\nval_dataset = TensorDataset(*inputs)\nval_loader = DataLoader(val_dataset, batch_size=500, shuffle=False)\n\n# --- 4. Run Evaluation ---\n# We use the function we defined in the previous step\nresults = evaluate_model(model, val_loader, threshold=0.8)\n\nprint(f\"Accuracy:  {results['accuracy']:.4f}\")\nprint(f\"Precision: {results['precision']:.4f}\")\nprint(f\"Recall:    {results['recall']:.4f}\")\nprint(f\"F1 Score:  {results['f1']:.4f}\")\nprint(\"\\nConfusion Matrix:\")\nprint(results['conf_matrix'])",
    "outputsMetadata": {
     "0": {
      "height": 185,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "#vocab_maps = build_vocab(train_df, emb_cols)\n",
    "\n",
    "inputs = preprocess_data(val_df, vocab_maps)\n",
    "\n",
    "val_dataset = TensorDataset(*inputs)\n",
    "val_loader = DataLoader(val_dataset, batch_size=500, shuffle=False)\n",
    "\n",
    "# --- 4. Run Evaluation ---\n",
    "# We use the function we defined in the previous step\n",
    "results = evaluate_model(model, val_loader, threshold=0.8)\n",
    "\n",
    "print(f\"Accuracy:  {results['accuracy']:.4f}\")\n",
    "print(f\"Precision: {results['precision']:.4f}\")\n",
    "print(f\"Recall:    {results['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {results['f1']:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(results['conf_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f9ad49-22d8-4a13-9d31-958dc7edfe9d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 11609,
    "lastExecutedAt": 1769385631598,
    "lastExecutedByKernel": "78867116-bed0-4f87-b19b-2b0f2b4479d5",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#vocab_maps = build_vocab(train_df, emb_cols)\n\ninputs = preprocess_data(test_df, vocab_maps)\n\ntest_dataset = TensorDataset(*inputs)\ntest_loader = DataLoader(test_dataset, batch_size=500, shuffle=False)\n\n# --- 4. Run Evaluation ---\n# We use the function we defined in the previous step\nresults = evaluate_model(model, test_loader, threshold=0.5)\n\nprint(f\"Accuracy:  {results['accuracy']:.4f}\")\nprint(f\"Precision: {results['precision']:.4f}\")\nprint(f\"Recall:    {results['recall']:.4f}\")\nprint(f\"F1 Score:  {results['f1']:.4f}\")\nprint(\"\\nConfusion Matrix:\")\nprint(results['conf_matrix'])",
    "outputsMetadata": {
     "0": {
      "height": 185,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "#vocab_maps = build_vocab(train_df, emb_cols)\n",
    "\n",
    "inputs = preprocess_data(test_df, vocab_maps)\n",
    "\n",
    "test_dataset = TensorDataset(*inputs)\n",
    "test_loader = DataLoader(test_dataset, batch_size=500, shuffle=False)\n",
    "\n",
    "# --- 4. Run Evaluation ---\n",
    "# We use the function we defined in the previous step\n",
    "results = evaluate_model(model, test_loader, threshold=0.5)\n",
    "\n",
    "print(f\"Accuracy:  {results['accuracy']:.4f}\")\n",
    "print(f\"Precision: {results['precision']:.4f}\")\n",
    "print(f\"Recall:    {results['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {results['f1']:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(results['conf_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e75467-8fd0-4e8b-989a-1fa6255b00fa",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 16,
    "lastExecutedAt": 1769383941318,
    "lastExecutedByKernel": "78867116-bed0-4f87-b19b-2b0f2b4479d5",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import chi2_contingency\n\ndef analyze_malware_shift(df_val, df_test, vocab_maps, col_name='processId'):\n    \"\"\"\n    Compares the distribution of a feature between Validation and Test sets,\n    specifically for Malicious samples.\n    \"\"\"\n    print(f\"--- Analyzing Feature: {col_name} ---\")\n    \n    # 1. Filter for MALICIOUS samples only (sus_label == 1)\n    # We only care if the attacks look different, not the safe traffic.\n    val_mal = df_val[(df_val['sus_label'] == 1)].copy()\n    test_mal = df_test[df_test['sus_label'] == 1].copy()\n    \n    # 2. Map the Raw Values to Indices (using the Training Vocab)\n    # This simulates exactly what the model sees (0 = Unknown)\n    val_indices = val_mal[col_name].apply(lambda x: vocab_maps[col_name].get(x, 0))\n    test_indices = test_mal[col_name].apply(lambda x: vocab_maps[col_name].get(x, 0))\n    \n    # 3. VISUALIZATION: Histograms üìä\n    plt.figure(figsize=(12, 5))\n    \n    # Validation Plot\n    plt.subplot(1, 2, 1)\n    plt.hist(val_indices, bins=20, color='blue', alpha=0.7, label='Validation')\n    plt.title(f\"Validation Set (Malicious Only)\\n{col_name} Indices\")\n    plt.xlabel(\"Index (0 is Unknown)\")\n    plt.ylabel(\"Count\")\n    plt.legend()\n    \n    # Test Plot\n    plt.subplot(1, 2, 2)\n    plt.hist(test_indices, bins=20, color='red', alpha=0.7, label='Test')\n    plt.title(f\"Test Set (Malicious Only)\\n{col_name} Indices\")\n    plt.xlabel(\"Index (0 is Unknown)\")\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 4. HYPOTHESIS TEST: Chi-Squared üìê\n    # Null Hypothesis (H0): The proportion of \"Unknowns\" (Index 0) is the same.\n    # Alt Hypothesis (H1): The proportion of \"Unknowns\" is different (Drift occurred).\n    \n    # Count [Unknowns, Knowns] for both sets\n    val_zeros = (val_indices == 0).sum()\n    val_known = (val_indices > 0).sum()\n    \n    test_zeros = (test_indices == 0).sum()\n    test_known = (test_indices > 0).sum()\n    \n    # Contingency Table\n    #           Unknown | Known\n    # Validation [  A   ,   B  ]\n    # Test       [  C   ,   D  ]\n    contingency = [[val_zeros, val_known], \n                   [test_zeros, test_known]]\n    \n    chi2, p, dof, expected = chi2_contingency(contingency)\n    \n    print(f\"Stats for {col_name}:\")\n    print(f\"  Validation Unknowns: {val_zeros} ({val_zeros/len(val_indices):.2%})\")\n    print(f\"  Test Unknowns:       {test_zeros} ({test_zeros/len(test_indices):.2%})\")\n    print(f\"  Chi-Squared p-value: {p:.5f}\")\n    \n    if p < 0.05:\n        print(\"  üö® SIGNIFICANT DISTRIBUTION SHIFT DETECTED!\")\n    else:\n        print(\"  ‚úÖ Distribution looks consistent.\")\n    print(\"-\" * 30)\n\n# --- Usage Example ---\n# analyze_malware_shift(df_val, df_test, vocab_maps, 'processId')"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def analyze_malware_shift(df_val, df_test, vocab_maps, col_name='processId'):\n",
    "    \"\"\"\n",
    "    Compares the distribution of a feature between Validation and Test sets,\n",
    "    specifically for Malicious samples.\n",
    "    \"\"\"\n",
    "    print(f\"--- Analyzing Feature: {col_name} ---\")\n",
    "    \n",
    "    # 1. Filter for MALICIOUS samples only (sus_label == 1)\n",
    "    # We only care if the attacks look different, not the safe traffic.\n",
    "    val_mal = df_val[(df_val['sus_label'] == 1)].copy()\n",
    "    test_mal = df_test[df_test['sus_label'] == 1].copy()\n",
    "    \n",
    "    # 2. Map the Raw Values to Indices (using the Training Vocab)\n",
    "    # This simulates exactly what the model sees (0 = Unknown)\n",
    "    val_indices = val_mal[col_name].apply(lambda x: vocab_maps[col_name].get(x, 0))\n",
    "    test_indices = test_mal[col_name].apply(lambda x: vocab_maps[col_name].get(x, 0))\n",
    "    \n",
    "    # 3. VISUALIZATION: Histograms üìä\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Validation Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(val_indices, bins=20, color='blue', alpha=0.7, label='Validation')\n",
    "    plt.title(f\"Validation Set (Malicious Only)\\n{col_name} Indices\")\n",
    "    plt.xlabel(\"Index (0 is Unknown)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Test Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(test_indices, bins=20, color='red', alpha=0.7, label='Test')\n",
    "    plt.title(f\"Test Set (Malicious Only)\\n{col_name} Indices\")\n",
    "    plt.xlabel(\"Index (0 is Unknown)\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. HYPOTHESIS TEST: Chi-Squared üìê\n",
    "    # Null Hypothesis (H0): The proportion of \"Unknowns\" (Index 0) is the same.\n",
    "    # Alt Hypothesis (H1): The proportion of \"Unknowns\" is different (Drift occurred).\n",
    "    \n",
    "    # Count [Unknowns, Knowns] for both sets\n",
    "    val_zeros = (val_indices == 0).sum()\n",
    "    val_known = (val_indices > 0).sum()\n",
    "    \n",
    "    test_zeros = (test_indices == 0).sum()\n",
    "    test_known = (test_indices > 0).sum()\n",
    "    \n",
    "    # Contingency Table\n",
    "    #           Unknown | Known\n",
    "    # Validation [  A   ,   B  ]\n",
    "    # Test       [  C   ,   D  ]\n",
    "    contingency = [[val_zeros, val_known], \n",
    "                   [test_zeros, test_known]]\n",
    "    \n",
    "    chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "    \n",
    "    print(f\"Stats for {col_name}:\")\n",
    "    print(f\"  Validation Unknowns: {val_zeros} ({val_zeros/len(val_indices):.2%})\")\n",
    "    print(f\"  Test Unknowns:       {test_zeros} ({test_zeros/len(test_indices):.2%})\")\n",
    "    print(f\"  Chi-Squared p-value: {p:.5f}\")\n",
    "    \n",
    "    if p < 0.05:\n",
    "        print(\"  üö® SIGNIFICANT DISTRIBUTION SHIFT DETECTED!\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ Distribution looks consistent.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# --- Usage Example ---\n",
    "# analyze_malware_shift(df_val, df_test, vocab_maps, 'processId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a5fea1-e5d7-4e65-ac52-7e0736435102",
   "metadata": {
    "executionCancelledAt": 1769379757667,
    "executionTime": 12,
    "lastExecutedAt": 1769377856274,
    "lastExecutedByKernel": "78867116-bed0-4f87-b19b-2b0f2b4479d5",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "train_df.columns"
   },
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f4ee80-8b40-4a7d-8259-824ae462e1ea",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 438,
    "lastExecutedAt": 1769383946196,
    "lastExecutedByKernel": "78867116-bed0-4f87-b19b-2b0f2b4479d5",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "analyze_malware_shift(val_df, test_df, vocab_maps, 'threadId')",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     },
     "2": {
      "height": 143,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "analyze_malware_shift(val_df, test_df, vocab_maps, 'threadId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b991ec9-85e0-4102-bd9c-6d1473edefc6",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 227,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# 1. Validation Data (Safe entries that look \"Unknown\")\n",
    "# We filter for Safe Label (0) but where the Parent ID mapped to 0\n",
    "safe_unknowns = val_df[(val_df['sus_label'] == 0) & (val_df['parentProcessId'].map(vocab_maps['parentProcessId']).fillna(0) == 0)]\n",
    "\n",
    "# 2. Test Data (Attacks that look \"Unknown\")\n",
    "# We filter for Malicious Label (1) where Parent ID mapped to 0\n",
    "malicious_unknowns = test_df[(test_df['sus_label'] == 1) & (test_df['parentProcessId'].map(vocab_maps['parentProcessId']).fillna(0) == 0)]\n",
    "\n",
    "# Compare their Arguments\n",
    "print(f\"Safe Unknowns - Avg Args: {safe_unknowns['argsNum'].mean():.2f}\")\n",
    "print(f\"Malicious Unknowns - Avg Args: {malicious_unknowns['argsNum'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718813e0-4d21-4e74-8fc2-3107b0d361c7",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 227,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "print(\"--- Safe Unknowns: Top User IDs ---\")\n",
    "# Show the top 5 most common users\n",
    "print(safe_unknowns['userId'].value_counts().head())\n",
    "\n",
    "print(\"\\n--- Malicious Unknowns: Top User IDs ---\")\n",
    "print(malicious_unknowns['userId'].value_counts().head())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "Cybr-Anom",
   "language": "python",
   "name": "cyber-anom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
